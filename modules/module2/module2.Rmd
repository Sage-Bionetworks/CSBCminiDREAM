---
title: "Module 2 - Introduction & basic modeling"
author: "Alex Perez and James Eddy"
date: "June 22, 2017"
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# About this activity

In this activity, we'll be showing you some basic operations for building statistical models in R. We'll also revisit some of the approaches from **Module 1** for inspecting and summarizing data — this time with a more specific focus on making sure our data is tidy and ready for modeling! At the end of the activity, you'll submit a new set of predicted survival times for the **mini-DREAM Challenge** — but this time, the predictions will be based on some of the clinical data you have available.

Just like **Module 0** and **Module 1**, you'll be reading through the notebook and running each code chunk to complete the activity. Read the explanations, look at each output, and think about what's happening. You won't need to understand how exactly the code or the stats are working, but try to get a feel for some of the cause and effect for different steps... and, as always, ask questions if anything is unclear!

---

# Preparing data for analysis

## Loading activity data

In the previous activity, you saw how data could be read into R from a file using one of several `read` functions. To make things a little simpler and quicker when you know you're going to be using the same data over and over again for an analysis, R also allows you to **save** data in `RData` files. In this case, we read the data from multiple tab-delimited `.txt` files using the `read.delim()` function, then saved the resulting data frames into a new file called `activity_data.RData`. Now, we can access these data frames simply using the `load()` command, without needing to worry as much about the specific format of the original file.

```{r}
load("/home/shared/data/metabric_split/activity_data.RData", verbose = TRUE)
```

That loaded two data frames (note the `df` abbreviation in the variable names): `activity_clinical_df` and `activity_expression_df`. The first includes clinical data for **1310** samples that we'll be using to explore concepts during the activity. The second table includes *gene expession* data, which we'll start working with in **Module 3**.

Last time, we saw how to use the `class()` function to check the data types for variables or columns in a table. R also has some handy functions for quickly summarizing the *structure* of objects, whether they're data frames, lists, vectors, or individual values. One of these is the `glimpse()` function that's available in a package named **`tibble`**, which includes lots of other useful tools for working with tables. **Note:** R has a very similar built-in function called `str()`, but the output of `glimpse()` can be a little easier to read in some cases. Let's try it out with `activity_clinical_df`:

```{r}
library(tibble)
glimpse(activity_clinical_df)
```

## Checking and fixing data types

As an analyst or data scientist preparing to build some models with this data, there are a few things one might notice. The data types for most of the 30 variables seem to match what we would expect. For example, `age_at_diagnosis` is a continuous variable, so R treats the values as `double` (or `dbl` for short) — you can generally think of `double` values as numerics with decimals, whereas `integer` values are whole numbers (see the `T` variable, which represents survival time in days). On the other hand, variables like `last_follow_up_status` and `histological_type` are represented as "characters", which is what we want for anything we might use to specificy different "categories". 

You might notice that none of the variables here are **factors** — we specifically told R not to convert anything into factors when reading in the data. This is OK! It's typically easier to create factors later on as needed, rather than try to change data from factors into a different type.

Unfortunately, there are some cases here where R has determined values are characters when a different type might be more appropriate. In particular, let's take a closer look at `lymph_nodes_positive`, `lymph_nodes_removed`, and `survival_5y`:


```{r}
# subset the table columns and glimpse
glimpse(activity_clinical_df[, c("lymph_nodes_positive", "lymph_nodes_removed", "survival_5y")])
```

The two `lymph_nodes` variables look like they should represent *counts* of something, and so we probably want these to be `integer` values. Meanwhile, `survival_5y` shows "True" and "False" as strings, but we know there's a better way in R to store these Boolean values: the `logical` data type.

For the `character` to `integer` conversion, there's a straightforward function we can use called `as.integer()`:

```{r}
head(activity_clinical_df$lymph_nodes_positive)
as.integer(head(activity_clinical_df$lymph_nodes_positive))
```

We can perform a similar conversion for `survival_5y` using the `as.logical()` function:

```{r}
head(activity_clinical_df$survival_5y)
as.logical(head(activity_clinical_df$survival_5y))
```

Now let's go ahead and convert the full columns in our data frame.

```{r}
# update lymph_nodes_positive
activity_clinical_df$lymph_nodes_positive <- as.integer(activity_clinical_df$lymph_nodes_positive)

# update lymph_nodes_removed
activity_clinical_df$lymph_nodes_removed <- as.integer(activity_clinical_df$lymph_nodes_removed)

# update survival_5y
activity_clinical_df$survival_5y <- as.logical(activity_clinical_df$survival_5y)
```

When we ran that code, we got two *warning* messages from R about "NAs" being introduced by coercion (coercion is the technical name for changing the type of a variable). While this kind of warning would typically require some follow-up investigation, we'll spoil the surprise for you: each of the `lymph_nodes` variables included the value `"null"` for some samples. So, what happens when we try to convert `"null"` into an integer?

```{r}
as.integer("null")
```

Basically, R doesn't know how to represent this string as a number, and decides to treat it as `NA` or "not available" instead. `NA` is a special type of value in R that is often used to indicate "missing data". Identifing and handling missing data is another important part of any statistical analysis. 

## Dealing with missing values

While there are lots of more sophisticated approaches to dealing with `NA`s in a dataset, we'll keep things simple for now and remove any variables that include `NA` values. In other words, if any of our clinical measurements or charateristics are unavailable for some samples, we won't consider those variables when building our models.

Before we identify and remove the variables with missing data, a few quick notes on `logical` values in R. We've mentioned that logical data types have some useful properties beyond what we get from "True" or "False" strings. One of these properties is for arithmetic, where `TRUE` is treated as `1` and `FALSE` as `0` for basic mathematical operations.

```{r}
TRUE + FALSE + TRUE
```

Why do we care about counting `TRUE` and `FALSE` values? Well, be using another handy function called `is.na()`, which asks "is this value `NA`?"

```{r}
is.na(c(NA, "foo"))
```

So, if we use `is.na()` to check all values in a column or vector, we'll get a resulting vector of `TRUE` or `FALSE` values. We can then count these logical values to quickly see how many `NA` or missing values we have in each column. Below, the `colSums()` lets us get these sums for *all* columns in the data frame.

```{r}
na_counts <- colSums(is.na(activity_clinical_df))
na_counts
```

OK — we can see that several variables have more than 0 missing values, and these are the columns we want to remove. To cut these columns, we'll use *another* nice property of logical values!

That is, logical values can be used to **index** (extract certain elements from) a table, list, or vector. If we have two vectors of the same length, you can think of each `TRUE` as saying "keep me" for the corresponding value at the same position, and each `FALSE` saying "drop me". For example, we can use the logical vector `c(TRUE, FALSE, TRUE)` to subset the integer vector `c(1, 2, 3)`. 

```{r}
x <- c(1, 2, 3)
x[c(TRUE, FALSE, TRUE)]
```

No more `2`! Back to our `NA` removal task... In `na_counts`, we have the count of missing values in each column of `activity_clinical_df`. We want to remove columns with counts greater than 0:

```{r}
na_counts > 0
```

If the `names()` function gives us the full list of column names in `activity_clinical_df`, then we can use `na_counts > 0` to subset this vector and show only those variables that include missing values.

```{r}
all_vars <- names(activity_clinical_df)
na_vars <- all_vars[na_counts > 0]
na_vars
```

With one more step, we'll pull out all of the columns that are **not** in the list of names above.

```{r}
activity_clinical_df[, !all_vars %in% na_vars]
```

Finally, we'll update `activity_clinical_df` to only include the columns with no missing data.

```{r}
activity_clinical_df <- activity_clinical_df[, !all_vars %in% na_vars]
glimpse(activity_clinical_df)
```

With our subsetted table and corrected data types, we're ready to move on to the next step!

---

# Plotting data for inspection

It can often be helpful to visualize data distributions with a plot. In this example we will explore two quick ways to visualize data that is continuous or discrete. We will use the age and stage fields in our data for this demonstration. The age data is continuous which we can see by simply plotting the unsorted values in the field and understanding there appears to be no pattern suggesting a discrete distribution of the data.

```{r}
plot(activity_clinical_df$age_at_diagnosis)
```

When we plot the stage data however, we clearly see a pattern suggestive of a discrete distribution.

```{r}
plot(activity_clinical_df$stage)
```

If we desire to get a more concrete view of how the age data is distributed we can use a combination of the **plot** function with the **density** distribution which will plot the data as a continuous function. This makes the **density** function an ideal way to visualize continuous data from our data frame.

```{r}
plot(density(activity_clinical_df$age_at_diagnosis))
```

Inspecting this plot suggests there is a slight bimodal distribution (i.e., a mixture of **two** underlying distributions) to the age of diagnosis. Take a minute to ponder what this bimodal distribution may suggest...

Now if we wish to get a more global view of the stage data, we want to take a different approach. Because the stage data is discrete, we want to utilize a plot that illustrates the discrete nature of the data. We can do this by combining the **table** and **barplot** commands. The **table** command enumerates the occurrences of each value in the stage data. The **barplot** function displays a barplot of the enumeration from the **table** command. Let's walk through an example to make this clear.

```{r}
barplot(table(activity_clinical_df$stage))
```

As we can see most of the data is composed of samples of stage 0. Knowing the demographics of our data is oftentimes crucial for any analysis of the data. Combining summary statistics with visualizations like those shown above represents a crucial first step in any analysis. 

---

# Modeling survival

## Introduction to modeling

After we have read in and inspected our data, we typically want to leverage it for some task. One task could be to build a predictive model from the data. These models can range from simple to complicated — and a model's use is very much dictated by the nature, quantity, and complexity of the data and task at hand. In general it is good practice to try and use the simplest model that accomplishes the task optimally. Among the workhorses of such models is the **linear regression**. 

Linear regression models take the canonical form of `y = mx + b`. In this classic model the dependent variable `y` is predicted by a single independent variable `x` which is multiplied by a *learned* slope coefficent `m`. The model intercepts the y-axis at point `b`. Now, this model can be expanded to have multiple independent variables with multiple learned `m` coefficents (multivariate linear regression), but for our purposes today we will simply try and model, from the data, the survival of a patient at 5 years (dependent variable) as a function of their age of diagnosis (independent variable).

To do this we will use R's **`lm()`** command which is the on-board **linear model** function avaliable in R. Let's start with an example and expand from there.

```{r}
lm(formula = activity_clinical_df$T ~ activity_clinical_df$age_at_diagnosis)
```

In this example the formula (y = mx + b) is a hardcoded word for the function which specifies the function to be used. The dependent variable is `activity_clinical_df$T` (survival time) and it is only the left side of the `~` which serves as the equality. On the right side of the `~` is the independent variables which in our case is just `activity_clinical_df$age_at_diagnosis`. The coefficient for the coefficent and the y-intercept are both learned or *trained* by the model through a error minimization procedure (method of least squares is one such method). We can make the command simpler by adding a data parameter which simply tells the linear model which object in the R enviroment represents the data. We'll also save our model to a new variable, so we can access it more easily.

```{r}
age_model <- lm(formula = T ~ age_at_diagnosis, data = activity_clinical_df)
age_model
```

## Inspecting and evaluating models

What does this output mean? Recall that the `lm()` function is trying to learn two parameters for a *line* that explains the trend of survival time (`T`) in relation to `age_at_diagnosis`: the intercept, `b`, and slope, `m`, such that we end up with the equation `T ~ m*age_at_diagnosis + b`. We can actually view this trend (and the fitted line) with another plot:

```{r}
plot(activity_clinical_df$age_at_diagnosis, activity_clinical_df$T)
abline(4055.79, -14.55)
```

The estimated slope or **coefficient** for `age_at_diagnosis` in the model, `-14.55` predicts that *for every 1-year increase in diagnosis age, survival time will decrease by about 14 days. This result (and the plot of the trend) probably aren't especially impressive, but let's see if we can evaluate our model in a more systematic way.

If we want to get an assessment for how good our model is there are a few things we can do. The first and simpliest is to simply use the **`summary()`** command again.

```{r}
summary(age_model)
```

This command will give a lot of informative output, but a quick way to assess the fit of our model is to look at the Adjusted R-squared value. This value represents the amount of variance in the data that is captured by our model. Just looking at age accounts for 1.1% of the variance in our model, which is pretty small. When we inspect the p-value associated with age as a covariate we see that it is highly significant... but this really only tells us that the relationship between diagnosis age and time probably isn't zero (even if it's small).

## Adding more information to our model

If we take a closer look at the scatterplot above, we can see that patients who are either on the young end or old end of the diagnosis age range tend to have lower survival times, creating almost a hump-shaped trend in the data. We might be able to use this observation to get a slightly better predictor for survival time.

First, we can use the `quantile()` function to find the *percentiles* for certain ages in the data. For example, the 5% or 5th-percentile diagnosis age says that "only 5% of values fall below this age"; in contrast the 95% or 95th-percentile says "95% of values fall below this age" (which means that only 5% fall above it). This is a useful way of finding the "extreme" ranges in a distribution. Note that the 0% and 100% quantiles represent the minimum and maximum value, respectively.

```{r}
age_ranges <- quantile(activity_clinical_df$age_at_diagnosis, 
                      probs = c(0, 0.05, 0.95, 1))
age_ranges
```

We can define a new variable called `diagnosis_age_extreme`, which is a *categorical* variable indicating whether or not patients were very young or very old at the time of diagnosis.

```{r}
activity_clinical_df$diagnosis_age_extreme <- activity_clinical_df$age_at_diagnosis < 40 | activity_clinical_df$age_at_diagnosis > 80
activity_clinical_df$diagnosis_age_extreme
```

Now, let's see what happens when we predict survival time based on whether patients were either very young or very old at the time of diagnosis.

```{r}
age_group_model <- lm(T ~ diagnosis_age_extreme, data = activity_clinical_df)
summary(age_group_model)
```

Some takeaways from this model: the estimated coefficient for `diagnosis_age_extreme == TRUE` is much larger in magnitude. In other words, being diagnosed either before 40 years old or after 80 years old is predicted to be associated with a decrease in survival time of over 944 day (nearly 3 years)! The p-value estimated for this coefficient is also much smaller, indicating more confidence that the effect is non-zero. Finally, our adjusted R-squared value has increased to ~0.03, indicating that our model explains about 3% of the variance in the data. This still isn't great, but it's a 300% improvement from looking at the overall age trend!

Instead of a scatterplot, we can use a **boxplot** to view the difference in survival times between patients with extreme diagnosis ages (`TRUE`) and everyone else (`FALSE`).

```{r}
boxplot(T ~ diagnosis_age_extreme, data = activity_clinical_df)
```

---

# mini-DREAM Challenge

Now that you've built a model to predict survival time based on diagnosis age group, we can apply it to the samples in a new dataset and submit our predictions to the **mini-DREAM Challenge**.

## Predicting survival time

Again, for the mini-DREAM challenge submissions, we'll be using a separate, special dataset where *we don't know* the survival time (we do know, but it will be hidden in the activities). We've made another "RData" file with this dataset to make it easier to load.

```{r}
load("/home/shared/data/metabric_split/challenge_data.RData", verbose = TRUE)
```

Because our model predicts survival time based on whether or not patients fall into the extreme groups for diagnosis age, we'll need to add this information to the challenge data frame.

```{r}
challenge_clinical_df$diagnosis_age_extreme <- challenge_clinical_df$age_at_diagnosis < 40 | challenge_clinical_df$age_at_diagnosis > 80
```

Now, we can use the `predict.lm()` function, which will use the coefficient and estimate we learned from the `activity_clinical_df` data above, apply this information to the `diagnosis_age_extreme` variable in the challenge data frame, and predict survival time for this cohort of 434 patients.

```{r}
prediction <- data.frame(METABRIC_ID = challenge_clinical_df$METABRIC_ID, 
                         T = predict.lm(age_group_model, 
                                        newdata = challenge_clinical_df))
prediction
```

## Submitting the prediction

You're now ready to submit the prediction. Just run the chunk below, a file with your prediction will be uploaded to Synapse and submitted to the challenge. You'll be able to see the results of your prediction on the mini-DREAM scoreboards, with the submission ID that gets printed below.

```{r}
library(synapseClient)
synapseLogin(rememberMe = TRUE)
submission_filename <- paste(Sys.getenv("USER"), "activity-2.txt", sep = "_")
write.csv(prediction, submission_filename)
activity2_submission <- synStore(File(path = submission_filename, parentId = "syn10088799"))
submission <- submit(evaluation = "9604686", entity = activity2_submission)
print(submission[[1]]$id)
```

---

# Wrapping up

Congrats — you’ve reached the end of **Module 2**! You can now return to the **mini-DREAM Challenge** site on Synapse.